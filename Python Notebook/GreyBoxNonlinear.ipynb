{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Greybox System Identification \n",
    "\n",
    "In this notebook we attempt to do greybox type of nonlinear system identification. The starting point is the dynamic given by Thomas Price and John Butt in *Catalyst Poisoning and Fixed Bed Reactor Dynamic II: Adiabatic Reactors*. \n",
    "The starting assumptions are the following:\n",
    "1. The input to the dynamic is the inlet temperature, the aromatic flow rate, the sulfur flow rate.\n",
    "2. Diffusion dynamic on planar axis is neglected.\n",
    "3. Partial differentiation with respect to horizontal axis is approximated by its backward numerical differentiation,\n",
    "   i.e., \n",
    "   $$\\left. \\dfrac{\\partial T}{\\partial z}\\right|_{z=z_i} \\approx \\frac{1}{\\Delta z} \\left[T(z_i) - T(z_{i-1})\\right],\\\\\n",
    "     \\left.\\dfrac{\\partial^2 T}{\\partial^2 z}\\right|_{z=z_i} \\approx \\frac{1}{\\Delta z} \\left[\\left. \\dfrac{\\partial T}{\\partial z}\\right|_{z=z_i} - \\left. \\dfrac{\\partial T}{\\partial z}\\right|_{z=z_{i-1}}\\right],\\\\\n",
    "     \\approx \\frac{1}{(\\Delta z)^2}\\left[T(z_i) - 2T(z_{i-1}) + T(z_{i-2})\\right]\\\\\n",
    "   $$\n",
    "\n",
    "The pde of the catalyst dynamic is given by:\n",
    "$$\n",
    "\\dfrac{d s_i}{dt} = K_0 \\dfrac{\\partial^2 s_i}{\\partial^2 z} -K_1 \\dfrac{\\partial s_i}{\\partial z}  - K_2 r_s(s_i,T_i) c_i,\\\\\n",
    "\\dfrac{d a_i}{dt} = K_3 \\dfrac{\\partial^2 a_i}{\\partial^2 z} -K_1 \\dfrac{\\partial a_i}{\\partial z} - K_2 r_a(a_i,T_i,c_i) c_i,\\\\\n",
    "\\dfrac{d T_i}{dt} = K_4 \\dfrac{\\partial^2 T_i}{\\partial^2 z} -K_5 \\dfrac{\\partial T_i}{\\partial z} + K_6 r_a(a_i,T_i,c_i) c_i,\\\\\n",
    "\\dfrac{d c_i}{dt} = K_7 r_s(s_i,T_i) c_i,\n",
    "$$\n",
    "   \n",
    "\n",
    "\n",
    "Let the reactor is partitioned into $N$ layers, where for each, the temperature corresponds to that layer is measured. The state each layer is given by (all is normalized):\n",
    "1. $s_i$ Sulfur concentration.\n",
    "2. $a_i$ Aromatic concentration.\n",
    "3. $T_i$ Temperature.\n",
    "4. $c_i$ Catalyst activity.\n",
    "\n",
    "where here $i = 1, \\cdots, N$.\n",
    "\n",
    "The inputs to the reactor are given by\n",
    "1. $s_0$ Sulfur concentration at inlet.\n",
    "2. $a_0$ Aromatic concentration at inlet.\n",
    "3. $T_0$ Temperature inlet.\n",
    "\n",
    "For each $i = 1, \\cdots, N$ the dyanmics are expressed as follows:\n",
    "$$\n",
    "\\dfrac{d s_i}{dt} = K_0 \\dfrac{1}{(\\Delta z)^2} \\left[s_i(t) -2 s_{i-1}(t) + s_{i-2}(t)\\right] -K_1 \\dfrac{1}{\\Delta z} \\left[s_i(t) - s_{i-1}(t)\\right]  - K_2 r_s(s_i,T_i) c_i,\\\\\n",
    "\\dfrac{d a_i}{dt} = K_3 \\dfrac{1}{(\\Delta z)^2} \\left[a_i(t) -2 a_{i-1}(t) + a_{i-2}(t)\\right] -K_1 \\dfrac{1}{\\Delta z} \\left[a_i(t) - a_{i-1}(t)\\right] - K_2 r_a(a_i,T_i,c_i) c_i,\\\\\n",
    "\\dfrac{d T_i}{dt} = K_4 \\dfrac{1}{(\\Delta z)^2} \\left[T_i(t) -2 T_{i-1}(t) + T_{i-2}(t)\\right] -K_5 \\dfrac{1}{\\Delta z} \\left[T_i(t) - T_{i-1}(t)\\right] + K_6 r_a(a_i,T_i,c_i) c_i,\\\\\n",
    "\\dfrac{d c_i}{dt} = K_7 r_s(s_i,T_i) c_i,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "r_a = -d_1 \\exp\\left(-d_2/T_i\\right) a_i c_i,\\\\\n",
    "r_s = -d_3 \\exp\\left(-d_4/T_i\\right)s_i %\\dfrac{d_3 \\exp\\left((d_4-d_5)/T_i\\right)s_i}{1+d_6 \\exp(d_4/T_i)s_i}.\n",
    "$$\n",
    "\n",
    "In these dynamical equations, the constants $K_j$ and $d_j$ belongs to $\\mathbb{R}^+$, to be estimated from the data via nonlinear optimization algorithm. We do not include noises model in the dynamics nor in the measurement model. We would handle this via stochastic smoothing or Gaussian processes system identification. These dynamics will be the starting point. It is assumed that initially $c_i = 1, \\forall i$.\n",
    "\n",
    "We then discretize the dynamics above in time, resulting in the following dynamics:\n",
    "$$\n",
    "s_i(t+1) = s_i(t)+K_0 \\dfrac{1}{(\\Delta z)^2} \\left[s_i(t) -2 s_{i-1}(t) + s_{i-2}(t)\\right] -K_1 \\dfrac{1}{\\Delta z} \\left[s_i(t) - s_{i-1}(t)\\right]  - K_2 r_s(s_i,T_i) c_i,\\\\\n",
    "a_i(t+1) = a_i(t)+K_3 \\dfrac{1}{(\\Delta z)^2} \\left[a_i(t) -2 a_{i-1}(t) + a_{i-2}(t)\\right] -K_1 \\dfrac{1}{\\Delta z} \\left[a_i(t) - a_{i-1}(t)\\right] - K_2 r_a(a_i,T_i,c_i) c_i,\\\\\n",
    "T_i(t+1) = T_i(t)+K_4 \\dfrac{1}{(\\Delta z)^2} \\left[T_i(t) -2 T_{i-1}(t) + T_{i-2}(t)\\right] -K_5 \\dfrac{1}{\\Delta z} \\left[T_i(t) - T_{i-1}(t)\\right] + K_6 r_a(a_i,T_i,c_i) c_i,\\\\\n",
    "c_i(t+1) = c_i(t)+ K_7 r_s(s_i,T_i) c_i,\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "We could start $u(t,z_i) = K_4 r_a(s_i(t),T_i(t)) c_i(t)$.\n",
    "\n",
    "$$\n",
    "c_i = c_i(0) \\exp(\\int r_s(t) dt)\\\\\n",
    "u_i = -r_a(a_i,T_i,c_i) c_i(0) \\exp(\\int r_s(t) dt)\\\\\n",
    "    = d_1 \\exp\\left(-d_2/T_i\\right) a_i c_i(0)^2 \\exp(2 \\int r_s(t) dt)\\\\\n",
    "    = d_1 \\exp\\left(-d_2/T_i\\right) \\exp(\\alpha_i(t,z_i)) c_i(0)^2 \\exp(2 \\int r_s(t,z_i) dt)\\\\\n",
    "\\log(u_i) = \\log(d_1) -d_2/T_i + \\alpha_i(t,z_i) + 2\\log(c_i(0)) + 2 \\int r_s(t,z_i) dt\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "Dtype = torch.float\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define discrete dynamics that will be used for system identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Discrete catalyst dynamics, with assumption that the number of layer is 7\n",
    "The diffusion take into account here\n",
    "'''\n",
    "# @torch.jit.script\n",
    "def catalyst_dynamics(x,u,n_levels,Delta_z,K,d):\n",
    "    #input are the log\n",
    "    d = torch.exp(d)\n",
    "    K = torch.exp(K)\n",
    "#     d = torch.max(d,torch.zeros_like(d))\n",
    "#     d = torch.max(K,torch.zeros_like(K))\n",
    "    \n",
    "    s_in = u[0]\n",
    "    a_in = u[1]\n",
    "    T_in = u[2]\n",
    "\n",
    "    #get the current states\n",
    "    s = x[:n_levels]\n",
    "    a = x[n_levels:2*n_levels]\n",
    "    T = x[2*n_levels:3*n_levels]\n",
    "    c = x[3*n_levels:]\n",
    "    \n",
    "    #create new state\n",
    "    x_new = torch.zeros_like(x)\n",
    "    s_new = x_new[:n_levels] #This is only view\n",
    "    a_new = x_new[n_levels:2*n_levels]\n",
    "    T_new = x_new[2*n_levels:3*n_levels]\n",
    "    c_new = x_new[3*n_levels:]\n",
    "    \n",
    "    for i in range(n_levels):\n",
    "        if i == 0:\n",
    "            s_i_min_1 = s_in\n",
    "            a_i_min_1 = a_in\n",
    "            T_i_min_1 = T_in\n",
    "            \n",
    "            s_i_min_2 = s_in\n",
    "            a_i_min_2 = a_in\n",
    "            T_i_min_2 = T_in\n",
    "        else:\n",
    "            s_i_min_1 = s[i-1]\n",
    "            a_i_min_1 = a[i-1]\n",
    "            T_i_min_1 = T[i-1]\n",
    "            if i==1:\n",
    "                s_i_min_2 = s_in\n",
    "                a_i_min_2 = a_in\n",
    "                T_i_min_2 = T_in\n",
    "            else:\n",
    "                s_i_min_2 = s[i-2]\n",
    "                a_i_min_2 = a[i-2]\n",
    "                T_i_min_2 = T[i-2]\n",
    "        \n",
    "        Delta_s = (s[i] - s_i_min_1)/Delta_z[i]\n",
    "        Delta_a = (a[i] - a_i_min_1)/Delta_z[i]\n",
    "        Delta_T = (T[i] - T_i_min_1)/Delta_z[i]\n",
    "        \n",
    "        \n",
    "        Delta2_s = (s[i] - 2*s_i_min_1 + s_i_min_2)/(Delta_z[i]*Delta_z[i])\n",
    "        Delta2_a = (a[i] - 2*a_i_min_1 + a_i_min_2)/(Delta_z[i]*Delta_z[i])\n",
    "        Delta2_T = (T[i] - 2*T_i_min_1 + T_i_min_2)/(Delta_z[i]*Delta_z[i])\n",
    "        \n",
    "        \n",
    "        rA_i = rA(s[i],T[i],d)\n",
    "        rS_i = rS(a[i],T[i],c[i],d)\n",
    "        \n",
    "        s_new[i] = s[i]+K[0]*Delta2_s - K[1]*Delta_s - K[2]*rS_i*c[i]\n",
    "        a_new[i] = a[i]+K[3]*Delta2_a - K[1]*Delta_a - K[2]*rA_i*c[i]\n",
    "        T_new[i] = T[i]+K[4]*Delta2_T - K[5]*Delta_T + K[6]*rA_i*c[i]\n",
    "        c_new[i] = c[i]+K[7]*rS_i\n",
    "        \n",
    "        \n",
    "    return x_new\n",
    "\n",
    "# @torch.jit.script\n",
    "def temperature_output(x,n_levels=7):\n",
    "    return x[2*n_levels:3*n_levels]\n",
    "\n",
    "'''\n",
    "Aromatic reaction rate\n",
    "'''\n",
    "# @torch.jit.script\n",
    "def rA(s,T,d):\n",
    "\n",
    "    res =  -d[2]*torch.exp(-d[3]/T)*s;\n",
    "#     res = res/(1+(d[4]*exp(d[3]/T)*s));\n",
    "    return res\n",
    "\n",
    "'''\n",
    "Sulphur reaction rate\n",
    "'''\n",
    "# @torch.jit.script\n",
    "def rS(a,T,c,d):\n",
    "    res =  -d[0]*torch.exp(-d[1]/T)*a*c;\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Discrete catalyst dynamics, where the dispersion is not assumed\n",
    "'''\n",
    "# @torch.jit.script\n",
    "def simple_catalyst_dynamics(x,u,n_levels,Delta_z,K,d):\n",
    "\n",
    "    #input are the log\n",
    "    d = torch.exp(d)\n",
    "    K = torch.exp(K)\n",
    "#     d = torch.max(d,torch.zeros_like(d))\n",
    "#     d = torch.max(K,torch.zeros_like(K))\n",
    "    \n",
    "    s_in = u[0]\n",
    "    a_in = u[1]\n",
    "    T_in = u[2]\n",
    "\n",
    "    #get the current states\n",
    "    s = x[:n_levels]\n",
    "    a = x[n_levels:2*n_levels]\n",
    "    T = x[2*n_levels:3*n_levels]\n",
    "    c = x[3*n_levels:]\n",
    "    \n",
    "    #create new state\n",
    "    x_new = torch.zeros_like(x)\n",
    "    s_new = x_new[:n_levels] #This is only view\n",
    "    a_new = x_new[n_levels:2*n_levels]\n",
    "    T_new = x_new[2*n_levels:3*n_levels]\n",
    "    c_new = x_new[3*n_levels:]\n",
    "    \n",
    "    for i in range(n_levels):\n",
    "        if i == 0:\n",
    "            s_i_min_1 = s_in\n",
    "            a_i_min_1 = a_in\n",
    "            T_i_min_1 = T_in\n",
    "            \n",
    "        else:\n",
    "            s_i_min_1 = s[i-1]\n",
    "            a_i_min_1 = a[i-1]\n",
    "            T_i_min_1 = T[i-1]\n",
    "            \n",
    "        \n",
    "        Delta_s = (s[i] - s_i_min_1)/Delta_z[i]\n",
    "        Delta_a = (a[i] - a_i_min_1)/Delta_z[i]\n",
    "        Delta_T = (T[i] - T_i_min_1)/Delta_z[i]\n",
    "        \n",
    "        \n",
    "        rA_i = rA(s[i],T[i],d)\n",
    "        \n",
    "        rS_i = rS(a[i],T[i],c[i],d)\n",
    "        \n",
    "        s_new[i] = s[i]- K[0]*Delta_s - K[1]*rS_i*c[i]\n",
    "        a_new[i] = a[i]- K[0]*Delta_a - K[1]*rA_i*c[i]\n",
    "        T_new[i] = T[i]- K[2]*Delta_T + K[3]*rA_i*c[i]\n",
    "        c_new[i] = c[i]+K[4]*rS_i\n",
    "        \n",
    "        \n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define costumized `Torch.nn` so that the parameter could be handled with `Torch.optim` algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Torch nn structure for easy manipulation of gradient and update\n",
    "class GreyIdentification(nn.Module):\n",
    "    def __init__(self,device,n_levels,initial_state,K_init,d_init,input_scale_init,simple=False):\n",
    "        super(GreyIdentification,self).__init__()\n",
    "        self.n_levels = n_levels\n",
    "        self.Delta_z = torch.tensor([1.,1.,1.,1.,1.,1.,1.],dtype=Dtype,device=device)\n",
    "        self.input_scale = nn.Parameter(data=input_scale_init,requires_grad=True)\n",
    "        self.K = nn.Parameter(data=K_init,requires_grad=True)\n",
    "        self.d = nn.Parameter(data=d_init,requires_grad=True)\n",
    "        self.initial_state = initial_state ##this can be estimated as well in the future\n",
    "#         self.state = initial_state.clone()#state at current time\n",
    "        self.useSimple = simple\n",
    "        if self.useSimple:\n",
    "            self.dynamics = simple_catalyst_dynamics\n",
    "        else:\n",
    "            self.dynamics = catalyst_dynamics\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    forward, given a history of input u, the network will predict the history of output y\n",
    "    '''\n",
    "    def forward(self,u_hist):\n",
    "        #first we need to sclae the input\n",
    "        u_hist = torch.cat((torch.exp(self.input_scale),torch.tensor([1.],device=device)))*u_hist\n",
    "        state = self.initial_state.clone()\n",
    "        output_hist = torch.empty(u_hist.shape[0],self.n_levels,dtype=Dtype,device=device)\n",
    "        output_hist[0,:] = temperature_output(state,self.n_levels)\n",
    "        \n",
    "        \n",
    "        for i in range(u_hist.shape[0]-1):\n",
    "            state = self.dynamics(state,u_hist[i,:],self.n_levels,self.Delta_z,self.K,self.d)\n",
    "            output_hist[i+1,:] = temperature_output(state,self.n_levels)\n",
    "            \n",
    "        return output_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Kate & Alex\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, n_levels=7, window=28, add_noise=False):\n",
    "        self.a_in = torch.from_numpy(df['AROM-LC wt-%'].values).float()\n",
    "        self.s_in = torch.from_numpy(df['sulphur feed max'].values).float()\n",
    "        self.temp_in = torch.from_numpy(df['TI8585'].values).float()\n",
    "        self.temp = torch.from_numpy(df[temperature_columns[0,1:-1]].values).float()\n",
    "               \n",
    "        self.dates = df.index\n",
    "        self.window = window\n",
    "        self.add_noise = add_noise\n",
    "    \n",
    "    def __len__(self):\n",
    "        times = self.a_in.shape[0]\n",
    "        return times - self.window + 1        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        noise_s = 0.00001 * torch.randn(self.window) if self.add_noise else 0.0\n",
    "        noise_a = 0.01 * torch.randn(self.window) if self.add_noise else 0.0\n",
    "        noise_temp = 0.01 * torch.randn(self.window) if self.add_noise else 0.0\n",
    "        return (self.s_in[idx:idx+self.window] + noise_s, \\\n",
    "               self.a_in[idx:idx+self.window] + noise_a, \\\n",
    "               self.temp_in[idx:idx+self.window] + noise_temp, \\\n",
    "               self.temp[idx:idx+self.window].squeeze(), \\\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_hdf('../Data/timeseries_complete.hdf5',key='KAAPO_hour_15_16_17_18_19_complete')\n",
    "df_raw = df_raw[(df_raw.index < \"2017-03-26\") & (df_raw.index > \"2015-07-14\")]\n",
    "df_lab = pd.read_hdf('../Data/Laboratory.hdf5',key='Laboratory').interpolate()\n",
    "df_lab = df_lab[(df_lab.index < \"2017-03-26\") & (df_lab.index > \"2015-07-14\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_columns = np.array(\n",
    "    [['TI8585','TI8553','TI8554','TI8555','TI8556','TI8557','TI8558','TI8559', 'TIZ8578A'],\n",
    "     ['TI8585','TI8560','TI8561','TI8562','TI8563','TI8564','TI8565','TI8566', 'TIZ8578A'],\n",
    "     ['TI8585','TI8567','TI8568','TI8569','TI8570','TI8571','TI8572','TI8573', 'TIZ8578A']],dtype=object)\n",
    "tc_heights =np.array([[7600,6550,5500,4450,3400,2350,1300],\n",
    "             [7250,6250,5150,4100,3050,2000,950],\n",
    "             [6900,5850,4800,3750,2700,1650,600]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select interval of interest and resample dataframe to daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_raw, df_lab], axis=1)\n",
    "df = df.resample('W').median() #resample daily or weekly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CustomDataset(df, n_levels=7, window=28, add_noise=False)\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_hist = torch.tensor(df[['AI8510A','Aromatic_percentage','TI8585']].values,device=device).float()\n",
    "u_hist = torch.tensor(df[['sulphur feed max','AROM-LC wt-%','TI8585']].fillna(0).values,device=device).float()\n",
    "y_hist = torch.tensor(df[temperature_columns[0,1:-1]].values,device=device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of layer\n",
    "layer_num = 7\n",
    "\n",
    "#set initial state\n",
    "s_init = u_hist[0,0]*torch.ones(layer_num,device=device,dtype=Dtype)\n",
    "a_init = u_hist[0,1]*torch.ones(layer_num,device=device,dtype=Dtype)\n",
    "T_init = y_hist[0,:]\n",
    "c_init = torch.ones(layer_num,device=device)\n",
    "initial_state = torch.cat((s_init,a_init,T_init,c_init))\n",
    "\n",
    "#initialize the parameter\n",
    "K_init = torch.log(torch.tensor([0.1090, 0.9871, 0.7476, 0.1604, 0.3059, 0.5792, 0.6542, 0.4632],dtype=Dtype,device=device))\n",
    "# K_init = torch.tensor([0.1090, 0.9871, 0.7476, 0.1604, 0.3059, 0.5792, 0.6542, 0.4632],dtype=Dtype,device=device)\n",
    "# K_init = torch.randn(8,dtype=Dtype,device=device)\n",
    "\n",
    "K_init_simple = torch.log(torch.tensor([0.9871, 0.7476, 0.5792, 0.6542, 0.4632],dtype=Dtype,device=device))\n",
    "# K_init_simple = torch.tensor([0.9871, 0.7476, 0.5792, 0.6542, 0.4632],dtype=Dtype,device=device)\n",
    "# K_init = torch.exp(torch.randn(8))\n",
    "\n",
    "d_init = torch.log(torch.tensor([0.2073, 0.1953, 0.8495, 0.8825],dtype=Dtype,device=device))\n",
    "# d_init = torch.tensor([0.2073, 0.1953, 0.8495, 0.8825],dtype=Dtype,device=device)\n",
    "# d_init = torch.tensor([0.2073, 0.1953, 0.8495, 0.8825],dtype=Dtype)\n",
    "\n",
    "input_scale_init = torch.randn(2,device=device)\n",
    "\n",
    "greyIdentification = GreyIdentification(device,7,initial_state,K_init_simple,d_init,input_scale_init,simple=True)\n",
    "# greyIdentification = GreyIdentification(device,7,initial_state,K_init,d_init,input_scale_init)\n",
    "greyIdentification.input_scale.data = torch.tensor([0.4789, 1.4224],dtype=Dtype,device=device)\n",
    "#Define loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "#use Adam at the moment for the optimizer\n",
    "# optimizer = torch.optim.Adam(greyIdentification.parameters(), lr=learning_rate)\n",
    "\n",
    "#use Adadelta\n",
    "optimizer = torch.optim.AdamW(greyIdentification.parameters(), lr=learning_rate)\n",
    "train_len = len(trainloader)\n",
    "epochs = 100\n",
    "# use scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=train_len, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check whether the dynamics with initial K_init and d_init can be runned through all timesteps without giving `nan` or `inf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_hist = torch.empty(u_hist.shape[0],28,device=device)\n",
    "state = initial_state.clone()\n",
    "state_hist[0,:] = state\n",
    "for i in range(u_hist.shape[0]-1):\n",
    "    state = greyIdentification.dynamics(state,u_hist[i,:],greyIdentification.n_levels,\\\n",
    "                                        greyIdentification.Delta_z,K_init,d_init)\n",
    "    state_hist[i+1,:] = state\n",
    "#     print(state[:layerNum])\n",
    "#     print(state[layerNum:2*layerNum])\n",
    "#     print(state[2*layer_num:3*layer_num])\n",
    "#     print(state[3*layer_num:])\n",
    "torch.any(torch.isnan(state_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f69df68a9d0>,\n",
       " <matplotlib.lines.Line2D at 0x7f69d5f55090>,\n",
       " <matplotlib.lines.Line2D at 0x7f69d5f55250>,\n",
       " <matplotlib.lines.Line2D at 0x7f69d5f55410>,\n",
       " <matplotlib.lines.Line2D at 0x7f69d5f555d0>,\n",
       " <matplotlib.lines.Line2D at 0x7f69d5f55790>,\n",
       " <matplotlib.lines.Line2D at 0x7f69d5f559d0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(state_hist.to('cpu')[:,3*layer_num:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss is 6355410.9677419355.\n",
      "Epoch 2: loss is 6155739.838709678.\n",
      "Epoch 3: loss is nan.\n",
      "Epoch 4: loss is nan.\n",
      "Epoch 5: loss is nan.\n",
      "Epoch 6: loss is nan.\n",
      "Epoch 7: loss is nan.\n",
      "Epoch 8: loss is nan.\n",
      "Epoch 9: loss is nan.\n",
      "Epoch 10: loss is nan.\n",
      "Epoch 11: loss is nan.\n",
      "Epoch 12: loss is nan.\n",
      "Epoch 13: loss is nan.\n",
      "Epoch 14: loss is nan.\n",
      "Epoch 15: loss is nan.\n",
      "Epoch 16: loss is nan.\n",
      "Epoch 17: loss is nan.\n",
      "Epoch 18: loss is nan.\n",
      "Epoch 19: loss is nan.\n",
      "Epoch 20: loss is nan.\n",
      "Epoch 21: loss is nan.\n",
      "Epoch 22: loss is nan.\n",
      "Epoch 23: loss is nan.\n",
      "Epoch 24: loss is nan.\n",
      "Epoch 25: loss is nan.\n",
      "Epoch 26: loss is nan.\n",
      "Epoch 27: loss is nan.\n",
      "Epoch 28: loss is nan.\n",
      "Epoch 29: loss is nan.\n",
      "Epoch 30: loss is nan.\n",
      "Epoch 31: loss is nan.\n",
      "Epoch 32: loss is nan.\n",
      "Epoch 33: loss is nan.\n",
      "Epoch 34: loss is nan.\n",
      "Epoch 35: loss is nan.\n",
      "Epoch 36: loss is nan.\n",
      "Epoch 37: loss is nan.\n",
      "Epoch 38: loss is nan.\n",
      "Epoch 39: loss is nan.\n",
      "Epoch 40: loss is nan.\n",
      "Epoch 41: loss is nan.\n",
      "Epoch 42: loss is nan.\n",
      "Epoch 43: loss is nan.\n",
      "Epoch 44: loss is nan.\n",
      "Epoch 45: loss is nan.\n",
      "Epoch 46: loss is nan.\n",
      "Epoch 47: loss is nan.\n",
      "Epoch 48: loss is nan.\n",
      "Epoch 49: loss is nan.\n",
      "Epoch 50: loss is nan.\n",
      "Epoch 51: loss is nan.\n",
      "Epoch 52: loss is nan.\n",
      "Epoch 53: loss is nan.\n",
      "Epoch 54: loss is nan.\n",
      "Epoch 55: loss is nan.\n",
      "Epoch 56: loss is nan.\n",
      "Epoch 57: loss is nan.\n",
      "Epoch 58: loss is nan.\n",
      "Epoch 59: loss is nan.\n",
      "Epoch 60: loss is nan.\n",
      "Epoch 61: loss is nan.\n",
      "Epoch 62: loss is nan.\n",
      "Epoch 63: loss is nan.\n",
      "Epoch 64: loss is nan.\n",
      "Epoch 65: loss is nan.\n",
      "Epoch 66: loss is nan.\n",
      "Epoch 67: loss is nan.\n",
      "Epoch 68: loss is nan.\n",
      "Epoch 69: loss is nan.\n",
      "Epoch 70: loss is nan.\n",
      "Epoch 71: loss is nan.\n",
      "Epoch 72: loss is nan.\n",
      "Epoch 73: loss is nan.\n",
      "Epoch 74: loss is nan.\n",
      "Epoch 75: loss is nan.\n",
      "Epoch 76: loss is nan.\n",
      "Epoch 77: loss is nan.\n",
      "Epoch 78: loss is nan.\n",
      "Epoch 79: loss is nan.\n",
      "Epoch 80: loss is nan.\n",
      "Epoch 81: loss is nan.\n",
      "Epoch 82: loss is nan.\n",
      "Epoch 83: loss is nan.\n",
      "Epoch 84: loss is nan.\n",
      "Epoch 85: loss is nan.\n",
      "Epoch 86: loss is nan.\n",
      "Epoch 87: loss is nan.\n",
      "Epoch 88: loss is nan.\n",
      "Epoch 89: loss is nan.\n",
      "Epoch 90: loss is nan.\n",
      "Epoch 91: loss is nan.\n",
      "Epoch 92: loss is nan.\n",
      "Epoch 93: loss is nan.\n",
      "Epoch 94: loss is nan.\n",
      "Epoch 95: loss is nan.\n",
      "Epoch 96: loss is nan.\n",
      "Epoch 97: loss is nan.\n",
      "Epoch 98: loss is nan.\n",
      "Epoch 99: loss is nan.\n",
      "Epoch 100: loss is nan.\n"
     ]
    }
   ],
   "source": [
    "#Set how many epoch?\n",
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for s_in, a_in, t_in, y in trainloader:\n",
    "        \n",
    "        a_in = a_in.to(device)\n",
    "        s_in = s_in.to(device)\n",
    "        t_in = s_in.to(device)\n",
    "        y = y.to(device).squeeze()\n",
    "        \n",
    "\n",
    "        # shape feed_in = (batch_size, n_times, 2)\n",
    "        u = torch.stack([s_in,s_in,t_in], dim=2).squeeze()\n",
    "        y_pred = greyIdentification(u)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(y_pred,y)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    scheduler.step(epoch_loss)\n",
    "    print(\"Epoch {0}: loss is {1}.\".format(ep + 1, epoch_loss / train_len))\n",
    "    \n",
    "    \n",
    "#     y_hist_pred = greyIdentification(u_hist)\n",
    "#     loss = loss_fn(y_hist,y_hist_pred)\n",
    "#     if ep % 10 == 9:\n",
    "#         print(ep, loss.item())\n",
    "\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # Backward pass: compute gradient of the loss with respect to model\n",
    "#     # parameters\n",
    "#     loss.backward(retain_graph=True)#but why\n",
    "\n",
    "#     # Calling the step function on an Optimizer makes an update to its\n",
    "#     # parameters\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     #call scheduler\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[204.7490, 204.8300, 199.4270, 207.7520, 207.4340, 206.9350, 207.6830],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
       "        [     nan,      nan,      nan,      nan,      nan,      nan,      nan]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greyIdentification.input_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-2.1636,  0.0270, -0.3220, -1.8716, -1.2107, -0.5198, -0.4577, -0.7313],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greyIdentification.K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-1.5408, -1.6528, -0.1959, -0.0939], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greyIdentification.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "    try:\n",
    "        do_save = input('Do you want to save the model (type yes to confirm)? ').lower()\n",
    "        if do_save == 'yes':\n",
    "            torch.save(model.state_dict(), filename)\n",
    "            print('Model saved to %s.' % (filename))\n",
    "        else:\n",
    "            print('Model not saved.')\n",
    "    except:\n",
    "        raise Exception('The notebook should be run or validated with skip_training=True.')\n",
    "\n",
    "\n",
    "def load_model(model, filename, device):\n",
    "    model.load_state_dict(torch.load(filename, map_location=lambda storage, loc: storage))\n",
    "    print('Model loaded from %s.' % filename)\n",
    "    model.to(device)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save the model (type yes to confirm)?  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not saved.\n"
     ]
    }
   ],
   "source": [
    "skip_training=T\n",
    "if not skip_training:\n",
    "    save_model(greyIdentification, 'greyIdentification.pth')\n",
    "    \n",
    "else:\n",
    "    n_levels = len(selected_cols)\n",
    "    model = CustomRnn(n_levels, use_true_params=False)\n",
    "    load_model(greyIdentification, 'greyIdentification.pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.tensor([1.,-1,0.]),torch.zeros(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
